{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.RS_dataset import RSLossConvDataset, collate_fn_rs_loss\n",
    "from models.rs_loss_models import RSLossPredictorResNet50\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Create datasets\n",
    "dataset_triangle = RSLossConvDataset(root_dir='shape_datasets/RS_loss_datasets', \n",
    "                       shape_type='triangle64sh_noisy', grid_size=40)\n",
    "dataset_quadrangle = RSLossConvDataset(root_dir='shape_datasets/RS_loss_datasets', \n",
    "                       shape_type='quadrangle64sh_noisy', grid_size=40)\n",
    "\n",
    "# combined_dataset = torch.utils.data.ConcatDataset([dataset_triangle, dataset_quadrangle])\n",
    "\n",
    "# Split dataset into train and test sets\n",
    "train_size = int(0.8 * len(dataset_triangle))  # 80% for training\n",
    "test_size = len(dataset_triangle) - train_size  # 20% for testing\n",
    "\n",
    "# Use random_split to create train and test datasets\n",
    "train_dataset_triangle, test_dataset_triangle = torch.utils.data.random_split(\n",
    "    dataset_triangle, \n",
    "    [train_size, test_size],\n",
    "    generator=torch.Generator().manual_seed(42)  # Set seed for reproducibility\n",
    ")\n",
    "train_dataset_quadrangle, test_dataset_quadrangle = torch.utils.data.random_split(\n",
    "    dataset_quadrangle, \n",
    "    [train_size, test_size],\n",
    "    generator=torch.Generator().manual_seed(42)  # Set seed for reproducibility\n",
    ")\n",
    "\n",
    "combined_train_dataset = torch.utils.data.ConcatDataset([train_dataset_triangle, train_dataset_quadrangle])\n",
    "combined_test_dataset = torch.utils.data.ConcatDataset([test_dataset_triangle, test_dataset_quadrangle])\n",
    "\n",
    "# Modify your DataLoader creation to use the collate function\n",
    "train_loader = DataLoader(\n",
    "    combined_train_dataset, \n",
    "    batch_size=16, \n",
    "    shuffle=True, \n",
    "    num_workers=15,\n",
    "    collate_fn=collate_fn_rs_loss\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    combined_test_dataset, \n",
    "    batch_size=16, \n",
    "    shuffle=False, \n",
    "    num_workers=15,\n",
    "    collate_fn=collate_fn_rs_loss\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning.pytorch import Trainer, seed_everything, callbacks\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "import lightning as L\n",
    "\n",
    "MAX_EPOCHS = 20\n",
    "MAX_STEPS = MAX_EPOCHS * len(train_loader)\n",
    "run_name = 'rs_loss_conv'\n",
    "\n",
    "# Initialize model and trainer]\n",
    "predictor = RSLossPredictorResNet50(learning_rate=1e-3, beta=0.00002, warmup_steps=1000, max_steps=MAX_STEPS)\n",
    "# checkpoint_path = 'model_weights/rs_loss_conv_combined.ckpt'\n",
    "# predictor = RSLossPredictorResNet50.load_from_checkpoint(checkpoint_path, max_steps=MAX_STEPS)\n",
    "\n",
    "trainer = L.Trainer(\n",
    "    max_epochs=MAX_EPOCHS,\n",
    "    accelerator='auto',\n",
    "    devices=1,\n",
    "    callbacks=[\n",
    "        callbacks.ModelCheckpoint(\n",
    "            monitor='val_loss',\n",
    "            dirpath='checkpoints',\n",
    "            filename='rs_loss-{epoch:02d}-{val_loss:.4f}',\n",
    "            save_top_k=3,\n",
    "            mode='min'\n",
    "        ),\n",
    "        callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=10,\n",
    "            mode='min'\n",
    "        )\n",
    "    ],\n",
    "    logger = TensorBoardLogger(\n",
    "        name='rs_conv_loss', \n",
    "        save_dir='./logs', \n",
    "        default_hp_metric=False,\n",
    "        version=run_name\n",
    "    ),\n",
    "    check_val_every_n_epoch = None,  # Disable validation every epoch\n",
    "    val_check_interval=4000  # Perform validation every 2000 training steps\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.validate(predictor, val_loader)\n",
    "trainer.fit(predictor, train_loader, val_loader)\n",
    "# Evaluate the model on the validation set\n",
    "trainer.validate(predictor, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save model weights\n",
    "checkpoint_path = f'model_weights/{run_name}.ckpt'\n",
    "trainer.save_checkpoint(checkpoint_path)\n",
    "print(f\"Model weights saved to {checkpoint_path}\")\n",
    "\n",
    "model_weights_path = f'model_weights/{run_name}.pt'\n",
    "torch.save(predictor.state_dict(), model_weights_path)\n",
    "print(f\"Model weights saved to {model_weights_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot learning curves\n",
    "from vizualization_utils import plot_learning_curves\n",
    "\n",
    "log_directory = './logs/rs_conv_loss'\n",
    "subdirs = [run_name]\n",
    "plot_learning_curves(log_directory, subdirs, metric='val_loss')\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
